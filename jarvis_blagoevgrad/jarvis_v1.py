import io
import re
import math
import pygame
import random
import spotipy
import requests
import subprocess
from googletrans import Translator
from langchain_ollama import OllamaLLM
import win32com.client as win32
from datetime import datetime, timedelta
import dateparser
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
import asyncio

from jarvis_functions.gemini_vision_method import *
from jarvis_functions.call_phone_method import *
from jarvis_functions.whatsapp_messaging_method import *
from jarvis_functions.ocr_model_method import *
from jarvis_functions.shazam_method import *

# Initialize Pygame
pygame.init()
pygame.mixer.init()
client = ElevenLabs(api_key="sk_a1f900fbd7f869b73954edc03d983b4fbebcfb597118b137")
r = sr.Recognizer()

#tv lights
WLED_IP = "192.168.10.211"

# Seting up spotify
client_id = 'dacc19ea9cc44decbdcb2959cd6eb74a'
client_secret = '11e970f059dc4265a8fe64aaa80a82bf'
sp = spotipy.Spotify(auth_manager=spotipy.SpotifyOAuth(
    client_id=client_id,
    client_secret=client_secret,
    redirect_uri='http://localhost:8888/callback',
    scope='user-library-read user-read-playback-state user-modify-playback-state'))  # Scope for currently playing song

jazz_playlist_url = "spotify:playlist/60joMYdXRjtwwfyERiGu4c?si=42cc553fb755446d"

# Setting up Gemini
os.environ["GEMINI_API_KEY"] = "AIzaSyBzMQutGJnduWwKcTrmvAvP_QiTj8zaJ3I"

genai.configure(api_key=os.environ["GEMINI_API_KEY"])

model = genai.GenerativeModel(model_name="gemini-1.5-flash")

system_instruction = (
    "–í–∏–µ —Å—Ç–µ –î–∂–∞—Ä–≤–∏—Å, –ø–æ–ª–µ–∑–µ–Ω –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–µ–Ω AI –∞—Å–∏—Å—Ç–µ–Ω—Ç."
    "–í–∏–Ω–∞–≥–∏ –æ—Ç–≥–æ–≤–∞—Ä—è–π—Ç–µ –ø—Ä–æ—Ñ–µ—Å–∏–æ–Ω–∞–ª–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ, –Ω–æ —Å—ä—â–æ —Å–µ –¥—Ä—ä–∂ –ø—Ä–∏—è—Ç–µ–ª—Å–∫–∏."
    "–ü–æ–¥–¥—ä—Ä–∂–∞–π—Ç–µ –æ—Ç–≥–æ–≤–æ—Ä–∏—Ç–µ –∫—Ä–∞—Ç–∫–∏, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏."
    "–û—Å–∏–≥—É—Ä–µ—Ç–µ, —á–µ –≤—Å–∏—á–∫–∏ –æ—Ç–≥–æ–≤–æ—Ä–∏ —Å–∞ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω–∏ –∏ –ª–µ—Å–Ω–∏ –∑–∞ —Ä–∞–∑–±–∏—Ä–∞–Ω–µ."
)

chat = model.start_chat(
    history=[
        {
            "role": "user",
            "parts": [system_instruction],
        }
    ]
)

# Create a Translator object
translator = Translator()

# Screen Dimensions
# info = pygame.display.Info()
# WIDTH, HEIGHT = info.current_w, info.current_h
# screen = pygame.display.set_mode((WIDTH, HEIGHT), pygame.FULLSCREEN)

WIDTH, HEIGHT = 1920, 1080
screen = pygame.display.set_mode((WIDTH, HEIGHT), pygame.RESIZABLE)
pygame.display.set_caption("Jarvis Interface")

# Colors
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)
BLUE = (0, 128, 255)
CYAN = (0, 255, 255)
ORANGE1 = (255, 165, 0)
ORANGE2 = (255, 115, 0)
GREEN1 = (0, 219, 0)
GREEN2 = (4, 201, 4)
PINK1 = (255, 182, 193)  # Light Pink
PINK2 = (255, 105, 180)  # Hot Pink
PURPLE1 = (166, 0, 255)
PURPLE2 = (176, 28, 255)
font_large = pygame.font.Font(None, 48)
font_small = pygame.font.Font(None, 32)

# Fonts
font_large = pygame.font.Font(pygame.font.get_default_font(), 36)
font_small = pygame.font.Font(pygame.font.get_default_font(), 20)

# Clock
clock = pygame.time.Clock()

# Rotating Circle Parameters
center = (WIDTH // 2, HEIGHT // 2)
max_radius = min(WIDTH, HEIGHT) // 3
angle = 0
speed = 1

# Particle Parameters
particles = []
num_particles = 100

# Pulse effect variables
pulse_factor = 1
pulse_speed = 0.05
min_size = 3
max_size = 3

# Color Transition
current_color_1 = list(BLUE)
current_color_2 = list(CYAN)
target_color_1 = list(BLUE)
target_color_2 = list(CYAN)
color_transition_speed = 10

jarvis_responses = [
    "–¢—É–∫ —Å—ä–º, –∫–∞–∫ –º–æ–≥–∞ –¥–∞ –ø–æ–º–æ–≥–Ω–∞?",
    "–°–ª—É—à–∞–º, –∫–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –∞—Å–∏—Å—Ç–∏—Ä–∞–º?",
    "–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –ø–æ–º–æ–≥–Ω–∞ –¥–Ω–µ—Å?",
    "–ö–∞–∫ –º–æ–≥–∞ –¥–∞ –í–∏ –ø–æ–º–æ–≥–Ω–∞?",
    "–î–∞",
    "–°–ª—É—à–∞–º"
]

selected_songs = [
    "Another one bites the dust - Queen",
    "Back in black",
    "Shoot to Thrill",
    "Thunderstruck",
    "You Give Love a Bad Name",
    "Highway to Hell - AC/DC",
    "September - Earth, Wind & Fire",
    "Should I Stay or Should I Go - Remastered",
    "If You Want Blood(You've Got It) - AC/DC",
    "Welcome T–æ The Jungle - Guns N' Roses"
]

status_list = []

jarvis_voice = "Brian" #deffault voice

# Ball initial random positions
random_particles = [{"x": random.randint(0, WIDTH), "y": random.randint(0, HEIGHT),
                     "dx": random.uniform(-2, 2), "dy": random.uniform(-2, 2)} for _ in range(num_particles)]

# State Variables
model_answering = False
is_collided = False
is_generating = False
wake_word_detected = False

running = True
current_song = ""
current_artist = ""
album_cover = None
current_progress = 0
song_duration = 0

def translate_input(user_input, direction="bg_to_en"):
    if direction == "bg_to_en":
        # Translate from Bulgarian to English
        translated_text = translator.translate(user_input, src='bg', dest='en')
        print(f"Original (BG): {user_input}")
        print(f"Translated (EN): {translated_text.text}")
    elif direction == "en_to_bg":
        # Translate from English to Bulgarian
        translated_text = translator.translate(user_input, src='en', dest='bg')
        print(f"Original (EN): {user_input}")
        print(f"Translated (BG): {translated_text.text}")

def set_volume(level):
    """Sets the system volume (Windows - using pycaw)."""
    try:
        devices = AudioUtilities.GetAudioEndpoints(CLSCTX_ALL, IAudioEndpointVolume)
        for device in devices:
            if device.IsDefaultAudioEndpoint(0):
                volume = device.Activate(IAudioEndpointVolume)
                level = max(0, min(100, level))
                volume.SetMasterVolumeLevelScalar(level / 100, None)
                print(f"Volume set to {level}%")
                return
        print("No default audio endpoint found") # Print if no default audio endpoint is found
    except Exception as e:
        print(f"Error setting volume: {e}")

def get_volume():
    """Gets the current system volume (Windows - using pycaw)."""
    try:
        devices = AudioUtilities.GetAudioEndpoints(CLSCTX_ALL, IAudioEndpointVolume)
        for device in devices:
            if device.IsDefaultAudioEndpoint(0):  # 0 for audio render role
                volume = device.Activate(IAudioEndpointVolume)
                current_volume = volume.GetMasterVolumeLevelScalar() * 100  # Convert to percentage
                return int(current_volume)  # Return as an integer
        return None  # Return None if no default audio endpoint is found
    except Exception as e:
        print(f"Error getting volume: {e}")
        return None

def mute():
    set_volume(0)  # Mute by setting volume to 0

def unmute():
    """Unmutes the system volume (sets it to a non-zero value)."""
    set_volume(50)  # Or any other non-zero value you prefer (e.g., 25, 75)
    print("System unmuted.")

def increase_volume(increment):
    current_volume = get_volume()
    if current_volume is None:
        print("Could not retrieve current volume, cannot increase")
        return
    set_volume(min(100, current_volume + increment))

def decrease_volume(decrement):
    current_volume = get_volume()
    if current_volume is None:
        print("Could not retrieve current volume, cannot decrease")
        return
    set_volume(max(0, current_volume - decrement))

def send_email(subject, body, to_email):
    outlook = win32.Dispatch('outlook.application')
    mail = outlook.CreateItem(0)
    mail.Subject = subject
    mail.Body = body
    mail.To = to_email
    mail.Send()

def parse_natural_time(natural_time):
    """
    Parses a natural language time expression (e.g., '3 —á–∞—Å–∞ —Å–ª–µ–¥–æ–±—è–¥ –¥–Ω–µ—Å', 'tomorrow', 'next Wednesday')
    into a datetime object.
    """

    # Manually handle '–¥–Ω–µ—Å' and '—É—Ç—Ä–µ' since dateparser fails sometimes
    now = datetime.now()

    # Replace Bulgarian words with English for better parsing
    normalized_time = (
        natural_time.replace("–¥–Ω–µ—Å", "today")
        .replace("—É—Ç—Ä–µ", "tomorrow")
        .replace("—Å–ª–µ–¥–æ–±—è–¥", "PM")
        .replace("—Å—É—Ç—Ä–∏–Ω—Ç–∞", "AM")
    )

    # Try parsing with dateparser
    event_time = dateparser.parse(
        normalized_time,
        languages=['bg', 'en'],  # Use both Bulgarian and English
        settings={'PREFER_DATES_FROM': 'future'}
    )

    # If dateparser fails, manually handle simple cases
    if event_time is None:
        if "–¥–Ω–µ—Å" in natural_time:
            event_time = now.replace(hour=15, minute=0, second=0, microsecond=0)
        elif "—É—Ç—Ä–µ" in natural_time:
            event_time = (now + timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
        else:
            raise ValueError(f"Could not parse the given time expression: {natural_time}")

    return event_time

def create_outlook_appointment(subject, start_time, duration):
    outlook = win32.Dispatch("Outlook.Application")
    appointment = outlook.CreateItem(1)  # 1 = olAppointmentItem

    appointment.Subject = subject
    appointment.Start = start_time
    appointment.Duration = duration
    appointment.ReminderMinutesBeforeStart = 15
    appointment.Save()

    print(f"‚úÖ Appointment '{subject}' scheduled for {start_time}")

def blend_color(current, target, speed):
    """Gradually transitions the current color toward the target color."""
    for i in range(3):
        diff = target[i] - current[i]
        if abs(diff) > speed:
            current[i] += speed if diff > 0 else -speed
        else:
            current[i] = target[i]

def draw_particles(surface, particles, target_mode=False):
    """Draws particles on the surface. If target_mode is True, arrange them in a circle and pulse."""
    global angle, pulse_factor

    for i, particle in enumerate(particles):
        if target_mode:
            # Calculate target circular positions
            target_x = center[0] + math.cos(math.radians(angle + i * 360 / len(particles))) * max_radius
            target_y = center[1] + math.sin(math.radians(angle + i * 360 / len(particles))) * max_radius
            # Smoothly move particles towards their circular positions
            particle["x"] += (target_x - particle["x"]) * 0.05
            particle["y"] += (target_y - particle["y"]) * 0.05

            # Pulse effect
            pulse_factor = min(max_size, pulse_factor + pulse_speed) if pulse_factor < max_size else max(min_size, pulse_factor - pulse_speed)
        else:
            # Move particles randomly when in default mode
            particle["x"] += particle["dx"]
            particle["y"] += particle["dy"]

            # Keep particles within the screen bounds
            if particle["x"] <= 0 or particle["x"] >= WIDTH:
                particle["dx"] *= -1
            if particle["y"] <= 0 or particle["y"] >= HEIGHT:
                particle["dy"] *= -1

        # Draw the particle
        pygame.draw.circle(surface, tuple(current_color_2), (int(particle["x"]), int(particle["y"])), int(pulse_factor))

def draw_response(model):
    """Update settings when the model is answering."""
    global target_color_1, target_color_2, is_collided, angle, speed

    if model == "Jarvis":
        target_color_1 = list(GREEN1)
        target_color_2 = list(GREEN2)
    elif model == "Friday":
        target_color_1 = list(PINK1)
        target_color_2 = list(PINK2)
    elif model == "Veronica":
        target_color_1 = list(PURPLE1)
        target_color_2 = list(PURPLE2)

    speed = 1
    is_collided = True
    angle += speed

def draw_thinking():
    """Update settings when the model is listening."""
    global target_color_1, target_color_2, is_collided, angle, speed
    target_color_1 = list(ORANGE1)
    target_color_2 = list(ORANGE2)
    speed = 0.5
    is_collided = True
    angle += speed

def draw_default():
    """Update settings when the model is not answering."""
    global target_color_1, target_color_2, is_collided, speed
    target_color_1 = list(BLUE)
    target_color_2 = list(CYAN)
    speed = 1
    is_collided = False

def draw_text(surface, text, position, font, color):
    """Draws text onto the surface."""
    text_surface = font.render(text, True, color)
    surface.blit(text_surface, position)

def extract_keywords(user_input):
    """
    Extract keywords from a natural language query.
    In this case, remove "can you search for" and return the rest.
    """
    trigger_phrase = "–ú–æ–∂–µ –ª–∏ –¥–∞ –ø–æ—Ç—ä—Ä—Å–∏—à –∑–∞"
    if user_input.lower().startswith(trigger_phrase):
        # Extract the part after the trigger phrase
        return user_input[len(trigger_phrase):].strip()
    return None

def perform_web_search(search_term):
    """
    Perform a web search using the extracted keywords.
    """
    if not search_term:
        print("No valid search term provided.")
        return
    # Encode the search term for the URL
    encoded_term = search_term.replace(" ", "+")
    # Construct the PowerShell command
    command = f'Start-Process "firefox.exe" "https://www.google.com/search?q={encoded_term}"'
    # Execute the PowerShell command
    subprocess.run(["powershell", "-Command", command], shell=True)

def fetch_current_track():
    """Fetch the current playing track and its album cover."""
    try:
        current_track = sp.currently_playing()
        if current_track and current_track['is_playing']:
            song = current_track['item']['name']
            artist = ", ".join([a['name'] for a in current_track['item']['artists']])
            album_cover_url = current_track['item']['album']['images'][0]['url']
            progress_ms = current_track['progress_ms']  # Progress in milliseconds
            duration_ms = current_track['item']['duration_ms']  # Duration in milliseconds
            return song, artist, album_cover_url, progress_ms, duration_ms
        return None, None, None, 0, 0
    except Exception as e:
        print(f"Error fetching track: {e}")
        return None, None, None, 0, 0

def load_album_cover(url):
    """Download and convert the album cover image to a Pygame surface."""
    try:
        response = requests.get(url)
        if response.status_code == 200:
            image_data = io.BytesIO(response.content)
            image = pygame.image.load(image_data, "jpg")
            return pygame.transform.scale(image, (300, 300))  # Scale to 300x300
    except Exception as e:
        print(f"Error loading album cover: {e}")
    return None

def draw_progress_bar(surface, x, y, width, height, progress, max_progress):
    """Draw a progress bar to represent the song timeline."""
    # Check if max_progress is non-zero to avoid division by zero
    if max_progress > 0:
        # Calculate the progress ratio
        progress_ratio = progress / max_progress
        progress_width = int(width * progress_ratio)
    else:
        progress_width = 0  # If duration is zero, show no progress

    # Draw the empty progress bar (background)
    pygame.draw.rect(surface, (50, 50, 50), (x, y, width, height))

    # Draw the filled progress bar (foreground)
    pygame.draw.rect(surface, GREEN1, (x, y, progress_width, height))

def set_color(red, green, blue):
    """Set the color using RGB values."""
    url = f"http://{WLED_IP}/json/state"
    data = {
        "on": True,
        "bri": 255,  # Optional: brightness
        "seg": [{
            "col": [[red, green, blue]]
        }]
    }
    response = requests.post(url, json=data)

def update_status(new_status):
    # Add new status to the list
    status_list.append(new_status)

    # Ensure the list only has 5 elements
    if len(status_list) > 5:
        status_list.pop(0)  # Remove the oldest status (first element)

def record_text():
    """Listen for speech and return the recognized text."""
    try:
        with sr.Microphone() as source:
            #print("Listening...")
            r.adjust_for_ambient_noise(source, duration=0.2)
            audio = r.listen(source)

            # Recognize speech using Google API
            MyText = r.recognize_google(audio, language="bg-BG")
            print(f"You said: {MyText}")
            return MyText.lower()

    except sr.RequestError as e:
        print(f"API Request Error: {e}")
        return None
    except sr.UnknownValueError:
        print("Sorry, I didn't catch that. Please try again.")
        return None

def chatbot():
    global wake_word_detected, model_answering, is_generating, current_model

    current_model= "Jarvis"

    print("Welcome to Vision! Say any of the models name to activate. Say 'exit' to quit.")

    while True:
        if not wake_word_detected:
            # Listen for the wake word
            print("Waiting for wake word...")
            user_input = record_text()

            if user_input and ("–¥–∂–∞—Ä–≤–∏—Å" in user_input or "–¥–∂–∞—Ä–≤–∏" in user_input or "–¥–∂–µ—Ä–≤–∏—Å" in user_input):
                wake_word_detected = True
                current_model = "Jarvis"
                pygame.mixer.music.load("sound_files/beep.flac")
                pygame.mixer.music.play()

                print("Wake word detected!")
                model_answering = True
                is_generating = False

                jarvis_voice = "Brian"
                response = random.choice(jarvis_responses)
                audio = client.generate(text=response, voice=jarvis_voice)
                play(audio)

                model_answering = False
                is_generating = True

            elif user_input and "friday" in user_input:
                wake_word_detected = True
                current_model = "Friday"
                pygame.mixer.music.load("sound_files/beep.flac")
                pygame.mixer.music.play()

                print("Wake word detected!")
                model_answering = True
                is_generating = False

                jarvis_voice = "Matilda"
                response = random.choice(jarvis_responses)
                audio = client.generate(text=response, voice=jarvis_voice)
                play(audio)

                model_answering = False
                is_generating = True

            elif user_input and "–í–µ—Ä–æ–Ω–∏–∫–∞" in user_input:
                wake_word_detected = True
                current_model = "Veronica"
                pygame.mixer.music.load("sound_files/beep.flac")
                pygame.mixer.music.play()

                print("Wake word detected!")
                model_answering = True
                is_generating = False

                jarvis_voice = "Sarah"
                response = random.choice(jarvis_responses)
                audio = client.generate(text=response, voice=jarvis_voice)
                play(audio)

                model_answering = False
                is_generating = True

            elif user_input == "–∏–∑–ª–µ–∑":
                print("Goodbye!")
                jarvis_voice = "Brian"
                audio = client.generate(text="Goodbye!", voice=jarvis_voice)
                play(audio)
                break

        else:
            # Actively listen for commands
            print("Listening for commands...")
            user_input = record_text()
            if user_input is None:
                print("Error: No input detected.")
                continue

            if "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏ —Å–µ" in user_input or "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—à" in user_input:
                audio = client.generate(text="–ó–¥—Ä–∞–≤–µ–π—Ç–µ, –∞–∑ —Å—ä–º –î–∂–∞—Ä–≤–∏—Å, –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª –Ω–∞ Gemini –æ–±—É—á–µ–Ω –æ—Ç Google."
                                             "–í–¥—ä—Ö–Ω–æ–≤–µ–Ω —Å—ä–º –æ—Ç –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω–∏—è –∏–∑–∫—É—Å—Ç–≤–µ–Ω –∏–Ω—Ç–µ–ª–µ–∫—Ç –Ω–∞ –¢–æ–Ω–∏ –°—Ç–∞—Ä–∫ ‚Äì –î–∂–∞—Ä–≤–∏—Å –æ—Ç –ñ–µ–ª–µ–∑–Ω–∏—è —á–æ–≤–µ–∫."
                                              "–ê–∑ —Å—ä–º —Ç—É–∫, –∑–∞ –¥–∞ –æ—Ç–≥–æ–≤–æ—Ä—è –Ω–∞ –≤—ä–ø—Ä–æ—Å–∏—Ç–µ –≤–∏, –¥–∞ –ø–æ–º–æ–≥–Ω–∞ —Å—ä—Å –∑–∞–¥–∞—á–∏ –∏–ª–∏ –¥–∞ –≤–æ–¥—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∏ –Ω–∞ –≤—Å—è–∫–∞–∫–≤–∏ —Ç–µ–º–∏. "
                                             "–ê–∫–æ –∏—Å–∫–∞—Ç–µ –¥–∞ –º–µ –ø–æ–ø–∏—Ç–∞—Ç–µ –Ω–µ—â–æ, –ø—Ä–æ—Å—Ç–æ –º–µ –ø–æ–≤–∏–∫–∞–π—Ç–µ –ø–æ –∏–º–µ.", voice="Brian")
                play(audio)
                model_answering = False

                wake_word_detected = True
                current_model = "Friday"
                model_answering = True
                is_generating = False
                audio = client.generate(text="–ó–¥—Ä–∞–≤–µ–π—Ç–µ, –∞–∑ —Å—ä–º Friday, –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª –Ω–∞ LLama3. "
                                             "–¢—É–∫ —Å—ä–º, –∑–∞ –¥–∞ –ø–æ–º–æ–≥–Ω–∞ —Å –≤—ä–ø—Ä–æ—Å–∏ –∏ –∑–∞–¥–∞—á–∏, –∫–∞—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∑–∞ –º–∞—à–∏–Ω–Ω–æ –æ–±—É—á–µ–Ω–∏–µ. "
                                              "–ê–∫–æ –∏—Å–∫–∞—Ç–µ –¥–∞ –º–µ –ø–æ–ø–∏—Ç–∞—Ç–µ –Ω–µ—â–æ, –ø—Ä–æ—Å—Ç–æ –º–µ –ø–æ–≤–∏–∫–∞–π—Ç–µ –ø–æ –∏–º–µ.", voice="Matilda")
                play(audio)
                model_answering = False

                wake_word_detected = True
                current_model = "Veronica"
                model_answering = True
                is_generating = False
                audio = client.generate(text="–ó–¥—Ä–∞–≤–µ–π—Ç–µ, –∞–∑ —Å—ä–º –í–µ—Ä–æ–Ω–∏–∫–∞, –µ–∑–∏–∫–æ–≤ –º–æ–¥–µ–ª –Ω–∞ DeepSeek. "
                                             "–ú–æ–µ—Ç–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –µ –¥–∞ –≤–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è–º —Ç–æ—á–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –¥–∞ –±—ä–¥–∞ –Ω–∞ —Ä–∞–∑–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∑–∞ –≤—Å—è–∫–∞–∫–≤–∏ –≤—ä–ø—Ä–æ—Å–∏ –∏–ª–∏ –∑–∞–¥–∞—á–∏, "
                                             "—Å –∫–æ–∏—Ç–æ —Å–µ –Ω—É–∂–¥–∞–µ—Ç–µ –æ—Ç –ø–æ–º–æ—â. –ê–∫–æ –∏—Å–∫–∞—Ç–µ –¥–∞ –º–µ –ø–æ–ø–∏—Ç–∞—Ç–µ –Ω–µ—â–æ, –ø—Ä–æ—Å—Ç–æ –º–µ –ø–æ–≤–∏–∫–∞–π—Ç–µ –ø–æ –∏–º–µ.", voice="Sarah")
                play(audio)
                model_answering = False

            if "–º–æ–∂–µ—à" in user_input and "–ø—Ä–∞–≤–∏—à " in user_input:
                audio = client.generate(text="–ú–æ–≥–∞ –¥–∞ —Ç—ä—Ä—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –¥–∞ —è –æ–±–æ–±—â–∞–≤–∞–º –∏ –¥–∞ –≤–∏ —è –ø—Ä–µ–¥—Å—Ç–∞–≤—è–º. "
                                             "–°—ä—â–æ —Ç–∞–∫–∞, –º–æ–≥–∞ –¥–∞ –∏–∑–ø—Ä–∞—â–∞–º –∏ —á–µ—Ç–∞ –∏–º–µ–π–ª–∏, –¥–∞ –ø—É—Å–∫–∞–º –º—É–∑–∏–∫–∞, –¥–∞ –æ—Ç–≤–∞—Ä—è–º –Ω–æ–≤–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –≤ Word –∏ –¥–æ—Ä–∏ –¥–∞ –≤–∏ –æ–ø–∏—à–∞ —Ç–æ–≤–∞, –∫–æ–µ—Ç–æ –≤–∏–∂–¥–∞–º.",
                                        voice="Brian")
                play(audio)

            if "–ø—É—Å–Ω–∏" in user_input and ("–ø–µ—Å–µ–Ω" in user_input or "–º—É–∑–∏–∫–∞" in user_input):
                audio = client.generate(text="–†–∞–∑–±–∏—Ä–∞ —Å–µ, –∏–º–∞—Ç–µ –ª–∏ –Ω—è–∫–∞–∫–≤–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–Ω–∏—è –∑–∞ –ø–µ—Å–µ–Ω?", voice=jarvis_voice)
                play(audio)

                print("Listening for song info...")
                user_input = record_text()

                if user_input is None:
                    audio = client.generate(text="–ùo –º–æ–∂–∞—Ö –¥–∞ —Ä–∞–∑–±–µ—Ä–∞. –ú–æ–∂–µ –ª–∏ –¥–∞ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ?", voice=jarvis_voice)
                    play(audio)
                    user_input = record_text()

                if "–¥–∞" in user_input:
                    audio = client.generate(text="–î–æ–±—Ä–µ, –∫–æ—è –ø–µ—Å–µ–Ω –±–∏—Ö—Ç–µ –∂–µ–ª–∞–ª–∏ –¥–∞ –≤–∏ –ø—É—Å–Ω–∞?",
                                            voice=jarvis_voice)
                    play(audio)

                    print("Listening for specific song...")
                    user_input = record_text()

                    audio = client.generate(text=f"–ü—É—Å–∫–∞–º, {user_input}",
                                            voice=jarvis_voice)
                    play(audio)
                    track_name = user_input
                    result = sp.search(q=track_name, limit=1)

                    # Get the song's URI
                    track_uri = result['tracks']['items'][0]['uri']
                    print(f"Playing track: {track_name}")

                    # Get the current device
                    devices = sp.devices()
                    # Find the LAPTOP_KOSI device by its ID
                    pc_device_id = '7993e31456b6d73672f9c7bcee055fb10ae52f23'
                    update_status(f"Played {track_name}")

                    # Start playback on the LAPTOP_KOSI device
                    sp.start_playback(device_id=pc_device_id, uris=[track_uri])
                    print("Playback started on LAPTOP_KOSI.")

                elif "–Ω–µ" in user_input:
                    audio = client.generate(text="–ü—É—Å–∫–∞–º —Ç–æ–≥–∞–≤–∞ –æ—Ç –∏–∑–±—Ä–∞–Ω–∏—è –æ—Ç –≤–∞—Å —Å–ø–∏—Å—ä–∫ —Å –ø–µ—Å–Ω–∏?",
                                            voice=jarvis_voice)
                    play(audio)

                    track_name = random.choice(selected_songs)
                    result = sp.search(q=track_name, limit=1)

                    # Get the song's URI
                    track_uri = result['tracks']['items'][0]['uri']
                    print(f"Playing track: {track_name}")

                    # Get the current device
                    devices = sp.devices()
                    # Find the LAPTOP_KOSI device by its ID
                    pc_device_id = '7993e31456b6d73672f9c7bcee055fb10ae52f23'
                    update_status(f"Played {track_name}")

                    # Start playback on the LAPTOP_KOSI device
                    sp.start_playback(device_id=pc_device_id, uris=[track_uri])
                    print("Playback started on LAPTOP_KOSI.")

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "–ø—Ä–∞—Ç–∏—à" in user_input and ("–∏–º–µ–π–ª" in user_input or "–ø–∏—Å–º–æ" in user_input):
                audio = client.generate(text="–†–∞–∑–±–∏—Ä–∞ —Å–µ, –∫—ä–º –∫–æ–≥–æ –±–∏—Ö—Ç–µ –∂–µ–ª–∞–ª–∏ –¥–∞ –ø—Ä–∞—Ç–∏—Ç–µ –∏–º–µ–π–ª–∞?", voice=jarvis_voice)
                play(audio)

                print("Listening for email info...")
                user_input = record_text()

                if "—Ç–∞—Ç–∏" in user_input or "–±–∞—â–∞ –º–∏" in user_input:
                    to_email = "bojidarbojinov@outlook.com"
                elif "–º–∞–º–∞" in user_input or "–º–∞–π–∫–∞ –º–∏" in user_input:
                    to_email = "kameliqbojinova@outlook.com"

                audio = client.generate(text="–ö–∞–∫–≤–∞ —â–µ –µ —Ç–µ–º–∞—Ç–∞ –Ω–∞ –≤–∞—à–µ—Ç–æ –ø–∏—Å–º–æ?", voice=jarvis_voice)
                play(audio)

                print("Listening for email info...")
                subject = record_text()

                audio = client.generate(text="–ö–∞–∫–≤–æ –∏—Å–∫–∞—Ç–µ –¥–∞ –∏–∑–ø—Ä–∞—Ç–∏—Ç–µ?", voice=jarvis_voice)
                play(audio)

                print("Listening for email info...")
                body = record_text()

                audio = client.generate(text="–°—É–ø–µ—Ä, –ø—Ä–µ–¥–∏ –¥–∞ –∏–∑–ø—Ä–∞—Ç—è –∏–º–µ–π–ª–∞, —â–µ –≤–∏ –∫–∞–∂–∞ –∫–∞–∫–≤–æ —Å—ä–º —Å–∏ –∑–∞–ø–∏—Å–∞–ª",
                                        voice=jarvis_voice)
                play(audio)

                if to_email == "bojidarbojinov@outlook.com":
                    audio = client.generate(text="–ò–º–µ–π–ª–∞ –µ –∫—ä–º –ë–æ–∂–∏–¥–∞—Ä –ë–æ–∂–∏–Ω–æ–≤ (–±–∞—â–∞ –≤–∏)", voice=jarvis_voice)
                    play(audio)
                elif to_email == "kameliqbojinova@outlook.com":
                    audio = client.generate(text="–ò–º–µ–π–ª–∞ –µ –∫—ä–º –ö–∞–º–µ–ª–∏—è –ë–æ–∂–∏–Ω–æ–≤–∞ (–º–∞–π–∫–∞ –≤–∏)", voice=jarvis_voice)
                    play(audio)
                audio = client.generate(text="–¢–µ–º–∞—Ç–∞ –Ω–∞ –ø–∏—Å–º–æ—Ç–æ –µ " + subject + "–ò —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ –ø–∏—Å–º–æ—Ç–æ –µ " + body,
                                        voice=jarvis_voice)
                play(audio)

                audio = client.generate(text="–í—Å–∏—á–∫–æ –Ω–∞—Ä–µ–¥ –ª–∏ –µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞ –≤ –ø–∏—Å–º–æ—Ç–æ?", voice=jarvis_voice)
                play(audio)

                print("Listening for approval...")
                user_input = record_text()

                if "–¥–∞" in user_input:
                    audio = client.generate(text="–°—É–ø–µ—Ä, –ø—Ä–∞—â–∞–º –∏–º–µ–π–ª–∞", voice=jarvis_voice)
                    play(audio)
                    send_email(subject, body, to_email)
                    update_status(f"Sent an email to {to_email}")

                elif "–Ω–µ" in user_input:
                    audio = client.generate(text="–°–æ—Ä–∫–∞", voice=jarvis_voice)
                    play(audio)

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "–ø—Ä–æ—á–µ—Ç–µ—à" in user_input and ("–ø–∏—Å–º–∞" in user_input or "–∏–º–µ–π–ª–∏" in user_input or "–ø–∏—Å" in user_input):
                # Initialize Outlook
                outlook = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
                inbox = outlook.GetDefaultFolder(6)  # 6 = Inbox

                # Get all messages sorted by received time (newest first)
                messages = inbox.Items
                messages.Sort("[ReceivedTime]", True)  # Sort descending (newest first)

                # Retrieve the last 5 emails
                num_emails = 3  # Change this number if you need more
                latest_messages = [messages.GetNext() for _ in range(num_emails)]

                audio = client.generate(text="–ï—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ç–µ 3 –∏–º–µ–π–ª–∞ –≤ –ø–æ—â–∞—Ç–∞ –≤–∏: ", voice=jarvis_voice)
                play(audio)
                # Print email details
                for i, email in enumerate(latest_messages, start=1):
                    print(f"\nüìß Email {i}:")
                    print(f"Subject: {email.Subject}")
                    print(f"From: {email.SenderName}")
                    print(f"Received: {email.ReceivedTime}")
                    print("\n--- Email Body ---\n")
                    print(email.Body)  # Full email body
                    print("\n--- End of Email ---\n")
                    audio = client.generate(text=f"–ò–º–µ–π–ª –Ω–æ–º–µ—Ä {i}, –∏–∑–ø—Ä–∞—Ç–µ–Ω–æ –µ –æ—Ç {email.SenderName}, "
                                                 f"—Ç–µ–º–∞—Ç–∞ –µ {email.Subject}, –∞ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ—Ç–æ –Ω–∞ –ø–∏—Å–º–æ—Ç–æ –µ {email.Body}", voice=jarvis_voice)
                    play(audio)

                update_status(f"Read last 3 emails")
                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if (("—Å—ä–±–∏—Ç–∏–µ" in user_input or "—Å—ä–±–∏—Ç–∏" in user_input or "—Å—ä–±–∏—Ç–∏—è" in user_input)
                    and ("—Å—ä–∑–¥–∞–¥–µ—à" in user_input or "–°—ä–∑–¥–∞–¥–µ—à" in user_input or "—Å—ä–∑–¥–∞–π" in user_input or "–°—ä–∑–¥–∞–π" in user_input)):
                # subject of event
                audio = client.generate(text="–†–∞–∑–±–∏—Ä–∞ —Å–µ, –∫–∞–∫ –∏—Å–∫–∞—Ç–µ –¥–∞ —Å–µ –∫–∞–∑–≤–∞ —Å—ä–±–∏—Ç–∏–µ—Ç–æ?", voice=jarvis_voice)
                play(audio)

                print("Listening for apointment info...")
                subject = record_text()

                # time of event
                audio = client.generate(text="–ó–∞ –∫–æ–≥–∞ –¥–∞ –±—ä–¥–µ —Ç–æ–≤–∞ —Å—ä–±–∏—Ç–∏–µ?", voice=jarvis_voice)
                play(audio)

                print("Listening for apointment info...")
                user_input = record_text()

                # duration of event
                audio = client.generate(text="–ö–æ–ª–∫–æ –≤—Ä–µ–º–µ —â–µ –ø—Ä–æ–¥—ä–ª–∂–∏ —Ç–æ–≤–∞ —Å—ä–±–∏—Ç–∏–µ?", voice=jarvis_voice)
                play(audio)

                print("Listening for apointment info...")
                duration = record_text()

                try:
                    event_time = parse_natural_time(user_input)
                    print(f"Parsed event time: {event_time}")  # Debug output
                    audio = client.generate(
                        text=f"–°—É–ø–µ—Ä, –∑–∞–ø–∞–∑–≤–∞–º —Å—ä–±–∏—Ç–∏–µ {subject}, –≤ {event_time.strftime('%H:%M %d-%m-%Y')}, –∏ —â–µ —Ç—Ä–∞–µ 1 —á–∞—Å",
                        voice=jarvis_voice)
                    play(audio)
                    create_outlook_appointment(subject, event_time, duration = 60)
                    update_status(f"Made an event")
                    model_answering = False
                    is_generating = False
                    wake_word_detected = False
                    continue
                except ValueError as e:
                    print(f"‚ùå Error: {e}")

                # –ù–∞–ø—Ä–∞–≤–∏ –º–∏ —Å—ä–±–∏—Ç–∏–µ –∑–∞ 3 —Å–ª–µ–¥–æ–±—è–¥ –¥–Ω–µ—Å, –∫–æ–µ—Ç–æ –¥–∞ –ø—Ä–æ–¥—ä–ª–∂–∏ 1 —á–∞—Å, –∏ –¥–∞ —Å–µ –∫–∞–∑–≤–∞ "–Ω–∞—Ö—Ä–∞–Ω–∏ –∫–æ—Ç–∫–∞—Ç–∞"pip install pywin32

            if ("–≤–∏–∂–¥–∞—à" in user_input or "–≤–∏–∂–¥–∞" in user_input) and "–∫–∞–∫–≤–æ" in user_input: # currently not working
                # # Open the webcam
                # audio = client.generate(text="–ö–∞–º–µ—Ä–∞—Ç–∞ –ø–æ –ø–æ–¥—Ä–∞–∑–±–∏—Ä–∞–Ω–µ –ª–∏ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º?", voice=jarvis_voice)
                # play(audio)
                #
                # print("Listening for camera info...")
                # #camera_info = record_text()
                # camera_info = input()
                #
                # if "–¥–∞" in camera_info:
                #     cap = cv2.VideoCapture(0)
                #     audio = client.generate(text="–î–æ–±—Ä–µ, –∏–∑–ø–æ–ª–∑–≤–∞–º web –∫–∞–º–µ—Ä–∞—Ç–∞ –Ω–∞ –∫–æ–º–ø—é—Ç—ä—Ä–∞ –≤–∏", voice=jarvis_voice)
                #     play(audio)
                # elif "–Ω–µ" in camera_info or "–¥—Ä—É–≥–∞—Ç–∞" in user_input:
                #     cap = cv2.VideoCapture(2)
                #     audio = client.generate(text="–î–æ–±—Ä–µ, –∏–∑–ø–æ–ª–∑–≤–∞–º –∫–∞–º–µ—Ä–∞—Ç–∞ –æ—Ç –≤–∏ –∞—Ä —Ö–µ–¥—Å–µ—Ç–∞",
                #                             voice=jarvis_voice)
                #     play(audio)
                #
                #
                # if not cap.isOpened():
                #     print("Error: Could not open webcam.")
                #     exit()
                #
                # # Create a named window
                # cv2.namedWindow("Capture Window", cv2.WINDOW_NORMAL)
                #
                # # Create a named window and resize it
                # cv2.namedWindow("Capture Window", cv2.WINDOW_NORMAL)
                # cv2.resizeWindow("Capture Window", 800, 600)  # Set window size to 800x600
                #
                # # Countdown from 3
                # for i in range(3, 0, -1):
                #     # Display the countdown on the OpenCV window
                #     ret, frame = cap.read()
                #     if not ret:
                #         print("Error: Failed to capture image.")
                #         break
                #
                #     # Add countdown text (centered)
                #     cv2.putText(frame, str(i), (350, 300), cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 0, 255), 5)
                #
                #     # Show frame
                #     cv2.imshow("Capture Window", frame)
                #     cv2.waitKey(1000)  # Wait for 1 second
                #
                # # Capture the final image when countdown hits 1
                # # pygame.mixer.music.load("camera_shutter.wav")
                # # pygame.mixer.music.play()
                # ret, frame_bgr = cap.read()
                # if not ret:
                #     print("Error: Failed to capture final image.")
                #     cap.release()
                #     cv2.destroyAllWindows()
                #     exit()
                #
                # # Convert BGR to RGB for Gemini
                # frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
                #
                # # Convert to PIL Image
                # captured_image = Image.fromarray(frame_rgb)
                #
                # # Close the OpenCV window
                # cap.release()
                # cv2.destroyAllWindows()
                #
                # # Provide a prompt
                # prompt = "–û–ø–∏—à–∏ –∫–∞–∫–≤–æ –≤–∏–∂–¥–∞—à –Ω–∞ —Å–Ω–∏–º–∫–∞—Ç–∞."
                #
                # # Send the image to the Gemini Vision model
                # response = model.generate_content([prompt, captured_image])
                #
                # # Print the AI's response
                # print("\nAI Response:")
                # print(response.text)
                #
                # audio = client.generate(text=response.text, voice=jarvis_voice)
                # play(audio)

                gemini_vision()
                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if ("–ø–∏—à–µ" in user_input or "–ø–∏—à–∞" in user_input) and "–∫–∞–∫–≤–æ" in user_input:
                # Open the webcam
                audio = client.generate(text="–ö–∞–º–µ—Ä–∞—Ç–∞ –ø–æ –ø–æ–¥—Ä–∞–∑–±–∏—Ä–∞–Ω–µ –ª–∏ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º?", voice=jarvis_voice)
                play(audio)

                print("Listening for camera info...")
                camera_info = record_text()

                camera_index = 0 #deffault index

                if "–¥–∞" in camera_info:
                    camera_index = 0
                    audio = client.generate(text="–î–æ–±—Ä–µ, –∏–∑–ø–æ–ª–∑–≤–∞–º web –∫–∞–º–µ—Ä–∞—Ç–∞ –Ω–∞ –∫–æ–º–ø—é—Ç—ä—Ä–∞ –≤–∏", voice=jarvis_voice)
                    play(audio)
                elif "–Ω–µ" in camera_info or "–¥—Ä—É–≥–∞—Ç–∞" in user_input:
                    camera_index = 1
                    audio = client.generate(text="–î–æ–±—Ä–µ, –∏–∑–ø–æ–ª–∑–≤–∞–º –∫–∞–º–µ—Ä–∞—Ç–∞ –æ—Ç –≤–∏ –∞—Ä —Ö–µ–¥—Å–µ—Ç–∞",
                                            voice=jarvis_voice)
                    play(audio)

                extracted_text_from_ocr = capture_and_ocr(camera_index = camera_index)

                if len(extracted_text_from_ocr) > 0:
                    print("Here is what the OCR model detected:")
                    print(extracted_text_from_ocr)
                    audio = client.generate(text=fr"OCR –º–æ–¥–µ–ª–∞ —Ä–∞–∑–ø–æ–∑–Ω–∞ —Å–ª–µ–¥–Ω–æ—Ç–æ {extracted_text_from_ocr}",
                                            voice=jarvis_voice)
                    play(audio)
                else:
                    print("Nothing was detected by the OCR model")
                    audio = client.generate(text="OCR –º–æ–¥–µ–ª–∞ –Ω–µ –º–æ–∂–∞ –¥–∞ —Ä–∞–∑–ø–æ–∑–Ω–∞–µ –Ω–∏—â–æ –æ—Ç —Å–Ω–∏–º–∫–∞—Ç–∞",
                                            voice=jarvis_voice)
                    play(audio)

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "–∑–≤—ä–Ω–Ω–µ—à" in user_input:
                call_phone()

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if ("—Å—ä–æ–±—â–µ–Ω–∏–µ" in user_input or "—Å—ä–æ–±—â–µ–Ω–∏—è" in user_input) and "–ø—Ä–∞—Ç–∏—à" in user_input:
                whatsapp_send_message()

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if ("—Ä–∞–∑–ø–æ–∑–Ω–∞–µ—à" in user_input or "–∫–æ—è" in user_input) and "–ø–µ—Å–µ–Ω" in user_input:
                audio = client.generate(text="–†–∞–∑–±–∏—Ä–∞ —Å–µ, –∑–∞–ø–æ—á–≤–∞–º –¥–∞ —Å–ª—É—à–∞–º. –ê–∫–æ —Ä–∞–∑–ø–æ–∑–Ω–∞—è –ø–µ—Å–µ–Ω—Ç–∞ —â–µ –≤–∏ –∫–∞–∂–∞ –∏–º–µ—Ç–æ –∏ –∞–≤—Ç–æ—Ä–∞ –Ω–∞ –ø–µ—Å–µ–Ω—Ç–∞",
                                        voice=jarvis_voice)
                play(audio)

                title, artist = recognize_audio()  # Get the title and artist
                if title and artist:
                    audio = client.generate(
                        text=f"–ù–∞–º–µ—Ä–∏—Ö –ø–µ—Å–µ–Ω—Ç–∞, –ø–µ—Å–µ–Ω—Ç–∞ –µ {title}, –∞ –∞–≤—Ç–æ—Ä–∞ –µ {artist}. –ñ–µ–ª–∞–µ—Ç–µ –ª–∏ –¥–∞ –ø—É—Å–Ω–∞ –ø–µ—Å–µ–Ω—Ç–∞ –≤ spotify?",
                        voice=jarvis_voice)
                    play(audio)
                    print(f"Song Title: {title}")
                    print(f"Artist: {artist}")

                    print("Listening for song info...")
                    answer_info = record_text()

                    if "–¥–∞" in answer_info:
                        audio = client.generate(text=f"–ü—É—Å–∫–∞–º, {title} –Ω–∞ {artist}",
                                                voice=jarvis_voice)
                        play(audio)
                        track_name = {title}
                        result = sp.search(q=track_name, limit=1)

                        # Get the song's URI
                        track_uri = result['tracks']['items'][0]['uri']
                        print(f"Playing track: {track_name}")

                        # Get the current device
                        devices = sp.devices()
                        # Find the LAPTOP_KOSI device by its ID
                        pc_device_id = '7993e31456b6d73672f9c7bcee055fb10ae52f23'
                        update_status(f"Played {track_name}")

                        # Start playback on the LAPTOP_KOSI device
                        sp.start_playback(device_id=pc_device_id, uris=[track_uri])
                        print("Playback started on LAPTOP_KOSI.")

                    elif "–Ω–µ" in answer_info:
                        model_answering = False
                        is_generating = False
                        wake_word_detected = False
                        continue
                else:
                    print("No song found")

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if (("–æ—Ç–≤–æ—Ä–∏" in user_input or "–æ—Ç–≤–æ—Ä–∏—à" in user_input or "–æ—Ç–≤–æ—Ä–∏—à" in user_input ) # currently not working
                    and ("word" in user_input or "wor" in user_input or "–¥–æ–∫—É–º–µ–Ω—Ç" in user_input)):
                audio = client.generate(text="–†–∞–∑–±–∏—Ä–∞ —Å–µ, –æ—Ç–≤–∞—Ä—è–º Word. –°–∞–º–æ —Å–µ–∫—É–Ω–¥–∞", voice=jarvis_voice)
                play(audio)

                word = win32.gencache.EnsureDispatch('Word.Application')
                word.Visible = True  # Optional: Make Word visible

                # Check if any documents are open. If not, create one.
                if word.Documents.Count == 0:
                    word.Documents.Add()  # Add a new document

                # *Crucial*: Wait a short time for Word to fully initialize and the document to open.
                time.sleep(2)  # Wait for 2 seconds (adjust as needed)

                selection = word.Selection

                audio = client.generate(text="–ì–æ—Ç–æ–≤ —Å—ä–º. –°–ª—É—à–∞–º –∏ –∑–∞–ø–∏—Å–≤–∞–º. –ö–∞–∂–µ—Ç–µ –¥—É–º–∞—Ç–∞ –ö—Ä–∞–π –∑–∞ –¥–∞ —Å–ø—Ä–∞ –¥–∞ –∑–∞–ø–∏—Å–≤–∞–º",
                                        voice=jarvis_voice)
                play(audio)

                while True:
                    with sr.Microphone() as source:
                        try:
                            print("Listening for...")
                            input_text = record_text()
                            print(f"You said: {input_text}")

                            # Stop listening when "end" is said
                            if input_text.lower() == "–∫—Ä–∞–π":
                                audio = client.generate(
                                    text="–°–ø—Ä—è—Ö –¥–∞ –∑–∞–ø–∏—Å–≤–∞–º, —Ñ–∞–π–ª–∞ –µ –∑–∞–ø–∞–∑–µ–Ω –≤ –ø–∞–ø–∫–∞ Downloads",
                                    voice=jarvis_voice)
                                play(audio)
                                break

                            selection.TypeText(input_text + ". ")

                            time.sleep(1)  # –ú–∞–ª–∫–æ –∑–∞–±–∞–≤—è–Ω–µ –∑–∞ —Ä–µ–∞–ª–∏–∑—ä–º

                        except sr.UnknownValueError:
                            print("Could not understand, try again.")
                        except sr.RequestError:
                            print("Speech recognition service error.")

                # # –ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                # doc.SaveAs(r"D:\downloads\proba1.docx")
                #
                # # Close Word
                # doc.Close()
                # word.Quit()

                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "–Ω–∞–º–∞–ª–∏" in user_input and "–∑–≤—É–∫–∞" in user_input:
                decrease_volume(10)
                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "—É—Å–∏–ª–∏" in user_input and "–∑–≤—É–∫–∞" in user_input:
                increase_volume(10)
                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "–∑–∞–≥–ª—É—à–∏" in user_input and "–∑–≤—É–∫–∞" in user_input:
                mute()
                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if "–æ—Ç–≥–ª—É—à–∏" in user_input and "–∑–≤—É–∫–∞" in user_input:
                unmute()
                model_answering = False
                is_generating = False
                wake_word_detected = False
                continue

            if user_input:
                # Start thinking state
                is_generating = True

                if (current_model == "Jarvis"): #Jarvis model (Gemini)
                    result = chat.send_message({"parts": [user_input]})

                elif (current_model == "Friday"):  # Friday model (Llama3)
                    model = OllamaLLM(model="llama3")

                    # Capture the translated text

                    translated_input = translate_input(user_input, direction="bg_to_en")

                    # Pass the translated input to the model

                    result = model.invoke(input=translated_input)

                elif (current_model == "Veronica"): #Friday model (Llama3)
                    model = OllamaLLM(model="deepseek-r1:1.5b")

                    # Translate the user input from Bulgarian to English
                    translated_input = translate_input(user_input, direction="bg_to_en")

                    # Build the full input for the model with the translated text
                    full_input = f"{system_instruction}\n\nUser: {translated_input}\nAssistant:"

                    # Get the model result
                    result1 = model.invoke(input=full_input)

                    # Remove the <think> part
                    result = re.sub(r"<think>.*?</think>", "", result1, flags=re.DOTALL)

                    print(result)  # For testing

                # Done generating the answer
                is_generating = False
                model_answering = True

                # Answering based on model
                if (current_model == "Jarvis"): #Jarvis answering
                    print(f"Jarvis: {result.text}")
                    audio = client.generate(text=result.text, voice=jarvis_voice)
                    play(audio)

                elif (current_model == "Friday"):  # Friday answering
                    print(f"FRIDAY: {result}")

                    # Translate the result from English to Bulgarian
                    translated_result = translate_input(result, direction="en_to_bg")

                    # Generate audio from the translated text
                    audio = client.generate(text=translated_result, voice=jarvis_voice)

                    # Play the generated audio
                    play(audio)

                elif (current_model == "Veronica"):  # Friday answering
                    print(f"Veronica: {result}")
                    # Translate the result from English to Bulgarian
                    translated_result = translate_input(result, direction="en_to_bg")

                    # Generate audio from the translated text
                    audio = client.generate(text=translated_result, voice=jarvis_voice)

                    # Play the generated audio
                    play(audio)

                model_answering = False
                is_generating = False

            wake_word_detected = False
# Main Loop
running = True
chatbot_thread = None

# Run chatbot in a separate thread
import threading
chatbot_thread = threading.Thread(target=chatbot)
chatbot_thread.start()

while running:
    screen.fill(BLACK)

    # Event Handling
    keys = pygame.key.get_pressed()
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False

    # Toggle behavior based on whether the model is generating or answering
    if is_generating:
        draw_thinking()  # Show thinking state
        #set_color(255, 165, 0)  # Orange
    elif model_answering:
        draw_response(current_model) # Show answering state
        #set_color(0, 219, 0)  # Green
    else:
        draw_default()  # Default state when nothing is happening.
        #set_color(0, 128, 255)  # Red

    # Smooth Color Transition
    blend_color(current_color_1, target_color_1, color_transition_speed)
    blend_color(current_color_2, target_color_2, color_transition_speed)

    # Draw Particles
    draw_particles(screen, random_particles, target_mode=is_collided)

    # Draw Text
    draw_text(screen, "Jarvis Interface", (10, 10), font_large, WHITE)
    draw_text(screen, "System Status: All Systems Online", (10, 60), font_small, tuple(current_color_2))

    # Draw the list of statuses under "System Status"
    start_y = 90  # Starting position for the list of items
    line_height = 30  # Space between each list item

    for index, status in enumerate(status_list):
        draw_text(screen, status, (10, start_y + index * line_height), font_small, WHITE)


    # Function to update the status list
    def update_status(new_status):
        # Add new status to the list and remove the oldest one if needed
        status_list.append(new_status)
        if len(status_list) > 5:  # Keep the list size manageable
            status_list.pop(0)

    # Fetch current track periodically (e.g., every 3 seconds)
    if pygame.time.get_ticks() % 3000 < 50:  # Update every 3 seconds
        song, artist, album_cover_url, progress_ms, duration_ms = fetch_current_track()
        if song and artist:  # Only update if song and artist are available
            current_song = song
            current_artist = artist
            current_progress = progress_ms
            song_duration = duration_ms

        # Draw the progress bar for the song timeline
        # Adjust the progress bar position if needed
    progress_bar_x = (WIDTH - 700) // 2
    progress_bar_y = HEIGHT - 30  # Adjust y-position for progress bar
    draw_progress_bar(screen, progress_bar_x, progress_bar_y, 700, 10, current_progress, song_duration)

    # Draw song information above the progress bar
    if current_song:
        song_surface = font_small.render(current_song, True, WHITE)
        song_text_x = (WIDTH - song_surface.get_width()) // 2
        song_text_y = progress_bar_y - 30  # Adjust y-position for song name
        screen.blit(song_surface, (song_text_x, song_text_y))

    # Update Display
    pygame.display.flip()
    clock.tick(60)

# Quit Pygame
pygame.quit()

